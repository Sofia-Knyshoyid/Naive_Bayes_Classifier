---
editor_options:
  markdown:
    wrap: 72
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Probability and Statistics

# Lab Assignment 1: Naive Bayes Classifier

### *Vinnik Tetiana, Sviatoslav Lushnei, Knishoid Sophia*

## Introduction

During the past three weeks, you learned a couple of essential notions
and theorems, and one of the most important among them is the *Bayes
theorem*.

One of its applications is **Naive Bayes classifier**, which is a
probabilistic classifier whose aim is to determine which class some
observation probably belongs to by using the Bayes formula:
$$\mathsf{P}(\mathrm{class}\mid \mathrm{observation})=\frac{\mathsf{P}(\mathrm{observation}\mid\mathrm{class})\mathsf{P}(\mathrm{class})}{\mathsf{P}(\mathrm{observation})}$$

Under the strong independence assumption, one can calculate
$\mathsf{P}(\mathrm{observation} \mid \mathrm{class})$ as
$$\mathsf{P}(\mathrm{observation}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i), \qquad \mathsf{P}(\mathrm{observation} \mid \mathrm{class}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i \mid \mathrm{class}),$$
where $n$ is the total number of features describing a given
observation. Thus, $\mathsf{P}(\mathrm{class}|\mathrm{observation})$ now
can be calculated as

$$\mathsf{P}(\mathrm{class} \mid \mathrm{\mathrm{observation}}) = \mathsf{P}(\mathrm{class})\times \prod_{i=1}^{n}\frac{\mathsf{P}(\mathrm{feature}_i\mid \mathrm{class})}{\mathsf{P}(\mathrm{feature}_i)}\tag{1}$$

All the terms on the right-hand side can be estimated from the data as
respective relative frequencies;\
see [this
site](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/)
for more detailed explanations.

## Data description

-   **2 - fake news** This data set contains data of American news: a
    headline and an abstract of the article. Each piece of news is
    classified as fake or credible. The task is to classify the news
    from test.csv as credible or fake.

Each data set consists of two files: *train.csv* and *test.csv*. The
first one you will need find the probabilities distributions for each of
the features, while the second one is needed for checking how well your
classifier works.

```{r}
# here goes a list of recommended libraries,
# though you may install other ones if they are needed
library(tidytext)
library(readr)
library(dplyr)
```

## Instructions

-   The first step is data pre-processing, which includes removing
    punctuation marks and stop words

-   represent each message as a bag-of-words

-   using the training set, calculate all the conditional probabilities
    in formula (1)

-   use those to predict classes for messages in the test set

-   evaluate effectiveness of the classifier by calculating the
    corresponding metrics

-   shortly summarize your work

-   do not forget to submit both the (compiled) Rmd source file and the
    .html output

### Data pre-processing

-   Read the *.csv* data files.
-   Ð¡lear your data from punctuation or other unneeded symbols.
-   Clear you data from stop words. You don't want words as is, and, or
    etc. to affect your probabilities distributions, so it is a wise
    decision to get rid of them. Find list of stop words in the cms
    under the lab task.
-   Represent each test message as its bag-of-words. Here:
    <https://machinelearningmastery.com/gentle-introduction-bag-words-model/>
    you can find general introduction to the bag-of-words model and
    examples on to create it.
-   It is highly recommended to get familiar with R dataframes, it would
    make the work much easier to do.
-   Useful links:
    -   <https://steviep42.github.io/webscraping/book/bagofwords.html#tidytext> -
        example of using *tidytext* to count frequencies of the words.
    -   Basics of Text Mining in R:
        <http://rstudio-pubs-static.s3.amazonaws.com/256588_57b585da6c054349825cba46685d8464.html>
        . Note that it also includes an example on how to create a bag
        of words from your text document.

```{r}
list.files(getwd())
list.files("/data/2-fake_news")
```

```{r}
naiveBayes <- setRefClass("naiveBayes",
       fields = list(all_data="data.frame", sum_fake="integer", sum_credible="integer", fake_stabilizer="numeric", cred_stabilizer="numeric"),
       methods = list(
         
                    fit = function(data)
                    {
                      data_fake <- train[train$Label == "fake",]
                      data_true <- train[train$Label == "credible",]
                      sum_fake <<- sum(data$Label=="fake")
                      sum_credible <<- sum(data$Label=="credible")
                      tidy_text_fake <- unnest_tokens(data_fake, 'splitted', 'Body', token="words")%>%
                      filter(!splitted %in% splitted_stop_words)
                      tidy_text_true <- unnest_tokens(data_true, 'splitted', 'Body', token="words")%>%
                      filter(!splitted %in% splitted_stop_words)
                      fake <- tidy_text_fake %>% count(splitted,sort=TRUE)
                      true <- tidy_text_true %>% count(splitted,sort=TRUE)
                      new_data <- fake %>% full_join(true, by="splitted")
                      new_data[is.na(new_data)] <- 0
                      num_col <- nrow(new_data)
                      colnames(new_data) <- c("splitted", "numFake", "numCred")
                      fake_stabilizer <<- sum(new_data$numFake) + num_col
                      cred_stabilizer <<- sum(new_data$numCred) + num_col
                      new_data$probFake <- (new_data$numFake+1)/fake_stabilizer
                      new_data$probCred <- (new_data$numCred+1)/cred_stabilizer
                      all_data <<- new_data
                    },
                    
                    # return prediction for a single message 
                    predict = function(message)
                    {
                        message <- tolower(message)
                        message <- gsub("[`[:punct:]]", '', message)
                        words = strsplit(message, split = "[[:space:]]")[[1]]
                        words <- words[!words %in% splitted_stop_words]
                        # get constant numbers
                        credToFakeRatio = sum_credible/sum_fake
                                              
                        for (word in words){
                            if (length(word)!=0) { 
                              
                                if ((word %in% all_data$splitted) && (all_data$numFake[all_data$splitted== word]>0))
                                    credToFakeRatio = credToFakeRatio / all_data$probFake[all_data$splitted == word]
                                else 
                                    credToFakeRatio = credToFakeRatio * fake_stabilizer
                                                
                                if ((word %in% all_data$splitted) && (all_data$numCred[all_data$splitted== word]>0))
                                    credToFakeRatio = credToFakeRatio * all_data$probCred[all_data$splitted == word]
                                else
                                    credToFakeRatio = credToFakeRatio / cred_stabilizer
                            }
                                                
                        }
                        if (credToFakeRatio > 1) 
                           return("credible")
                        return("fake")
                    },
                    
                    score = function(test_data)
                    {
                      test_data["predicted"] <- NA
                      for(i in 1:nrow(test_data)) {
                        message <- test_data$Body[i]
                        prediction <- model$predict(message)
                        test_data[i, "predicted"] = prediction
                      }
                      
                      test_data$results <- (test_data$Label == test_data$predicted)
                      print(head(test_data))
                      print(sum(test_data$results)/nrow(test_data))
                      return(test_data)
                      
                      test_data["fp"] = apply(test_data, 1, FUN = (function(x) return ((x["predicted"]=="credible") && (x["Label"]=="fake"))))
                      test_data["fn"] = apply(test_data, 1, FUN = (function(x) return ((x["predicted"]=="fake") && (x["Label"]=="credible"))))
                      test_data["tp"] = apply(test_data, 1, FUN = (function(x) return ((x["predicted"]=="credible") && (x["Label"]=="credible"))))
                      test_data["tn"] = apply(test_data, 1, FUN = (function(x) return ((x["predicted"]=="fake") && (x["Label"]=="fake"))))
                      fpr = sum(test_data["fp"])
                      tpr = sum(test_data["tp"])
                      fnr = sum(test_data["fn"])
                      tnr = sum(test_data["tn"])
                      accuracy = sum(test_data$results)/nrow(test_data)
                      precision = tpr/(tpr+fpr)
                      recall = tpr/(tpr+tnr)
                    }
))

model = naiveBayes()
model$fit(train)
dataf <- model$score(test)
dataf
```

```{r}
test_data <- dataf
test_data["fp"] = apply(test_data, 1, FUN = (function(x) return ((x["predicted"]=="credible") && (x["Label"]=="fake"))))
test_data["fn"] = apply(test_data, 1, FUN = (function(x) return ((x["predicted"]=="fake") && (x["Label"]=="credible"))))
test_data["tp"] = apply(test_data, 1, FUN = (function(x) return ((x["predicted"]=="credible") && (x["Label"]=="credible"))))
test_data["tn"] = apply(test_data, 1, FUN = (function(x) return ((x["predicted"]=="fake") && (x["Label"]=="fake"))))
fpr = sum(test_data["fp"])
tpr = sum(test_data["tp"])
fnr = sum(test_data["fn"])
tnr = sum(test_data["tn"])
accuracy = sum(test_data$results)/nrow(test_data)
precision = tpr/(tpr+fpr)
recall = tpr/(tpr+fnr)
f1score = 2*recall*precision/(recall+precision)
head(test_data)

print("Result accuracy:")
print(accuracy)
print("Result precision:")
print(precision)
print("Result recall:")
print(recall)
print("Result f1-score:")
print(f1score)
```

## Measure effectiveness of your classifier

-   Note that accuracy is not always a good metric for your classifier.
    Look at precision and recall curves, F1 score metric.
-   Visualize them.
-   Show failure cases.

## Conclusions
    
    To build NaiveClassifier we implemented fit (anylize train data and calculates conditional probabilities) predict(makes prediction on the inputted message by using        Bayes Rules and probability table) and score(calculates results for the whole test base) methods. 
    
    We can see that our model works really good on provided data (all measures are close to 0.96)
    Pros:
    It is easy to implement 
    Is takes  `few minutes` to train the model.
    It gives a good results in a short time.
    
    Cons:
    It is naive and don`t takes into account many important factors.
    We have too assume that data (words) are independent and have class distribution in general.
